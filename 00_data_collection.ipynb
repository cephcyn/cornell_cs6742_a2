{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c602396-cfaa-4840-8ad4-4128621b210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79736ac2-f4a8-4adb-86dc-04a0b9602fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit_posts(subreddit_name, scrape_year, backup_fname_in=None, backup_fname_out=None):\n",
    "    # checkpoint output\n",
    "    print(f'scraping subreddit: {subreddit_name}')\n",
    "    print(f'    data from year: {scrape_year}')\n",
    "    print(f' scrape START time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}', flush=True)\n",
    "    t0 = time.process_time()\n",
    "    scrape_begin = int(datetime(scrape_year, 1, 1).timestamp())\n",
    "    scrape_end = int(datetime(scrape_year+1, 1, 1).timestamp())\n",
    "    \n",
    "    # load from backup if available\n",
    "    if (backup_fname_in is not None) and exists(f'{backup_fname_in}.csv'):\n",
    "        df_sub = pd.read_csv(f'{backup_fname_in}.csv')\n",
    "        created_utc_now_sub = df_sub.tail(1)['created_utc']\n",
    "        current_start = int(created_utc_now_sub + 1)\n",
    "        print(f'Loaded checkpoint: {backup_fname_in}.csv')\n",
    "    else:\n",
    "        current_start = scrape_begin\n",
    "        df_sub = pd.DataFrame()\n",
    "        print(f'Started from scratch')\n",
    "    \n",
    "    # do the submissions scrape\n",
    "    keep_subscraping = True\n",
    "    while keep_subscraping:\n",
    "        try:\n",
    "            with urllib.request.urlopen(\n",
    "                f'https://api.pushshift.io/reddit/search/submission/'\n",
    "                +f'?limit=1000&sort=asc&subreddit={subreddit_name}&after={current_start}&before={scrape_end}'\n",
    "            ) as url:\n",
    "                data = json.loads(url.read().decode())\n",
    "        except Exception as e:\n",
    "            print('EXCEPTION HAPPENED:')\n",
    "            print(e, flush=True)\n",
    "            if backup_fname_out is not None:\n",
    "                df_sub.to_csv(f'{backup_fname_out}.csv', index=False)\n",
    "                print(f'Saved checkpoint: {backup_fname_out}.csv')\n",
    "            sys.exit()\n",
    "        if ('data' in data) and len(data['data'])>0:\n",
    "            data = data['data']\n",
    "            df_sub_new = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n",
    "            df_sub = df_sub.append(df_sub_new)\n",
    "            created_utc_now_sub = df_sub_new.tail(1)['created_utc']\n",
    "            current_start = int(created_utc_now_sub + 1)\n",
    "            print(f'new batch submissions count: {df_sub_new.shape[0]}, next scanning {current_start}-{scrape_end}', flush=True)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            keep_subscraping = False\n",
    "            \n",
    "    # checkpoint output\n",
    "    print(f'   scrape END time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}')\n",
    "    print(f'    total DURATION: {time.process_time() - t0}', flush=True)\n",
    "    \n",
    "    # return scraped data\n",
    "    df_sub = df_sub.reset_index(drop=True)\n",
    "    if backup_fname_out is not None:\n",
    "        df_sub.to_csv(f'{backup_fname_out}.csv', index=False)\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd7aee-8f5f-4ce2-8c0c-1738b73af8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'AcePlace'\n",
    "year = 2017\n",
    "df_sub = scrape_subreddit_posts(subreddit, year, backup_fname_in='temp_01', backup_fname_out='temp_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172815a-1026-4e01-88c5-d0959bc0ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_post_comments(subreddit_name, df_sub, backup_fname_in=None, backup_fname_out=None):\n",
    "#     # checkpoint output\n",
    "#     print(f'scraping given batch of posts for comment list')\n",
    "#     print(f' scrape START time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}', flush=True)\n",
    "#     t0 = time.process_time()\n",
    "    \n",
    "#     # load from backup if available\n",
    "#     if (backup_fname_in is not None) and exists(f'{backup_fname_in}.pkl'):\n",
    "#         with open(f'{backup_fname_in}.pkl', 'rb') as f:\n",
    "#             comment_ids = pickle.load(f)\n",
    "#         print(f'Loaded checkpoint: {backup_fname_in}.pkl')\n",
    "#     else:\n",
    "#         comment_ids = []\n",
    "#         print(f'Started from scratch')\n",
    "    \n",
    "#     # do the comments ID scrape\n",
    "#     for sub_id in df_sub['id'][len(comment_ids):]:\n",
    "#         try:\n",
    "#             with urllib.request.urlopen(\n",
    "#                 f'https://api.pushshift.io/reddit/submission/comment_ids/{sub_id}'\n",
    "#             ) as url:\n",
    "#                 data = json.loads(url.read().decode())\n",
    "#         except Exception as e:\n",
    "#             print('EXCEPTION HAPPENED:')\n",
    "#             print(e, flush=True)\n",
    "#             if backup_fname_out is not None:\n",
    "#                 with open(f'{backup_fname_out}.pkl', 'wb') as f:\n",
    "#                     pickle.dump(comment_ids, f)\n",
    "#                 print(f'Saved checkpoint: {backup_fname_out}.pkl')\n",
    "#             sys.exit()\n",
    "#         comment_ids.append(data['data'])\n",
    "#         if len(comment_ids)%50==0:\n",
    "#             print(f'got comments from these many posts so far: {len(comment_ids)}', flush=True)\n",
    "#     df_sub['comments'] = comment_ids\n",
    "#     print(f'should have {sum([len(x) for x in comment_ids])} comments total')\n",
    "            \n",
    "#     # checkpoint output\n",
    "#     print(f'   scrape END time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}')\n",
    "#     print(f'    total DURATION: {time.process_time() - t0}', flush=True)\n",
    "    \n",
    "#     # return scraped data\n",
    "#     df_sub = df_sub.reset_index(drop=True)\n",
    "#     if backup_fname_out is not None:\n",
    "#         with open(f'{backup_fname_out}.pkl', 'wb') as f:\n",
    "#             pickle.dump(comment_ids, f)\n",
    "#     return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777e333-449e-4b75-8445-7a90418e408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub = scrape_post_comments(subreddit, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9908b9-9864-4607-ac11-f4c6dcdc8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_full_comments(subreddit_name, df_sub, backup_fname_in=None, backup_fname_out=None):\n",
    "    # checkpoint output\n",
    "    print(f'scraping given batch of posts for comment details')\n",
    "    print(f' scrape START time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}', flush=True)\n",
    "    t0 = time.process_time()\n",
    "    \n",
    "    # load from backup if available\n",
    "    if (backup_fname_in is not None) and exists(f'{backup_fname_in}.csv'):\n",
    "        df_comm = pd.read_csv(f'{backup_fname_in}.csv')\n",
    "        print(f'Loaded checkpoint: {backup_fname_in}.csv')\n",
    "    else:\n",
    "        df_comm = pd.DataFrame()\n",
    "        print(f'Started from scratch')\n",
    "    \n",
    "    # do the comments detail scrape\n",
    "    for sub in df_sub.index:\n",
    "        sub_id = df_sub.loc[sub]['id']\n",
    "        num_comms = df_sub.loc[sub]['num_comments']\n",
    "        if ('link_id' in df_comm) and (f't3_{sub_id}' in [str(e) for e in df_comm['link_id']]):\n",
    "            continue\n",
    "        keep_commscraping = num_comms>0\n",
    "        current_start = 1\n",
    "        comms_so_far = 0\n",
    "        mini_comm = pd.DataFrame()\n",
    "        while keep_commscraping:\n",
    "            try:\n",
    "                with urllib.request.urlopen(\n",
    "                    f'https://api.pushshift.io/reddit/comment/search/'\n",
    "                    +f'?limit=1000&sort=asc&link_id={sub_id}&after={current_start}'\n",
    "                ) as url:\n",
    "                    data = json.loads(url.read().decode())\n",
    "            except Exception as e:\n",
    "                print('EXCEPTION HAPPENED:')\n",
    "                print(e, flush=True)\n",
    "                if backup_fname_out is not None:\n",
    "                    df_comm.to_csv(f'{backup_fname_out}.csv', index=False)\n",
    "                    print(f'Saved checkpoint: {backup_fname_out}.csv')\n",
    "                sys.exit()\n",
    "            if ('data' in data) and len(data['data'])>0:\n",
    "                data = data['data']\n",
    "                mini_comm_new = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n",
    "                mini_comm = mini_comm.append(mini_comm_new)\n",
    "                created_utc_now_comm = mini_comm_new.tail(1)['created_utc']\n",
    "                current_start = int(created_utc_now_comm + 1)\n",
    "                print(f'new batch comments count: {mini_comm_new.shape[0]}, next scanning {current_start}-?', flush=True)\n",
    "                comms_so_far += max(500, mini_comm_new.shape[0])\n",
    "                keep_commscraping = (comms_so_far < num_comms)\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                keep_commscraping = False\n",
    "        df_comm = df_comm.append(mini_comm)\n",
    "        print(f'finished indexing comments from one submission', flush=True)\n",
    "    print(f'comments count {df_comm.shape[0]}')\n",
    "            \n",
    "    # checkpoint output\n",
    "    print(f'   scrape END time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}')\n",
    "    print(f'    total DURATION: {time.process_time() - t0}', flush=True)\n",
    "    \n",
    "    # return scraped data\n",
    "    df_comm = df_comm.reset_index(drop=True)\n",
    "    if backup_fname_out is not None:\n",
    "        df_comm.to_csv(f'{backup_fname_out}.csv', index=False)\n",
    "    return df_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8dadc-7f2c-4ca1-93b8-4fa1caaa8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comm = scrape_full_comments(subreddit, df_sub, backup_fname_in='temp_02', backup_fname_out='temp_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850679a-121e-4b29-bc07-4db9ea4523f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b8f87-1fd4-490b-aaab-1da3c260d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2316d15-d4ce-4883-a3e4-d94ae7e0e717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
