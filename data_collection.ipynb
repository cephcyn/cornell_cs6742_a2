{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c602396-cfaa-4840-8ad4-4128621b210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import argparse\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79736ac2-f4a8-4adb-86dc-04a0b9602fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit_posts(subreddit_name, scrape_year):\n",
    "    # checkpoint output\n",
    "    print(f'scraping subreddit: {subreddit_name}')\n",
    "    print(f'    data from year: {scrape_year}')\n",
    "    print(f' scrape START time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}', flush=True)\n",
    "    t0 = time.process_time()\n",
    "    scrape_begin = int(datetime(scrape_year, 1, 1).timestamp())\n",
    "    scrape_end = int(datetime(scrape_year+1, 1, 1).timestamp())\n",
    "    \n",
    "    # do the submissions scrape\n",
    "    keep_subscraping = True\n",
    "    current_start = scrape_begin\n",
    "    df_sub = pd.DataFrame()\n",
    "    while keep_subscraping:\n",
    "        with urllib.request.urlopen(\n",
    "            f'https://api.pushshift.io/reddit/search/submission/'\n",
    "            +f'?limit=1000&sort=asc&subreddit={subreddit_name}&after={current_start}&before={scrape_end}'\n",
    "        ) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "        if ('data' in data) and len(data['data'])>0:\n",
    "            data = data['data']\n",
    "            df_sub_new = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n",
    "            df_sub = df_sub.append(df_sub_new)\n",
    "            created_utc_now_sub = df_sub_new.tail(1)['created_utc']\n",
    "            current_start = int(created_utc_now_sub + 1)\n",
    "            print(f'new batch submissions count: {df_sub_new.shape[0]}, next scanning {current_start}-{scrape_end}', flush=True)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            keep_subscraping = False\n",
    "            \n",
    "    # checkpoint output\n",
    "    print(f'   scrape END time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}')\n",
    "    print(f'    total DURATION: {time.process_time() - t0}', flush=True)\n",
    "    \n",
    "    # return scraped data\n",
    "    df_sub = df_sub.reset_index(drop=True)\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd7aee-8f5f-4ce2-8c0c-1738b73af8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'AcePlace'\n",
    "year = 2017\n",
    "df_sub = scrape_subreddit_posts(subreddit, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172815a-1026-4e01-88c5-d0959bc0ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post_comments(subreddit_name, df_sub):\n",
    "    # checkpoint output\n",
    "    print(f'scraping given batch of posts for comment list')\n",
    "    print(f' scrape START time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}', flush=True)\n",
    "    t0 = time.process_time()\n",
    "    \n",
    "    # do the comments ID scrape\n",
    "    comment_ids = []\n",
    "    for sub_id in df_sub['id']:\n",
    "        with urllib.request.urlopen(\n",
    "            f'https://api.pushshift.io/reddit/submission/comment_ids/{sub_id}'\n",
    "        ) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "        comment_ids.append(data['data'])\n",
    "        if len(comment_ids)%50==0:\n",
    "            print(f'got comments from these many posts so far: {len(comment_ids)}', flush=True)\n",
    "    df_sub['comments'] = comment_ids\n",
    "    print(f'should have {sum([len(x) for x in comment_ids])} comments total')\n",
    "            \n",
    "    # checkpoint output\n",
    "    print(f'   scrape END time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}')\n",
    "    print(f'    total DURATION: {time.process_time() - t0}', flush=True)\n",
    "    \n",
    "    # return scraped data\n",
    "    df_sub = df_sub.reset_index(drop=True)\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777e333-449e-4b75-8445-7a90418e408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = scrape_post_comments(subreddit, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9908b9-9864-4607-ac11-f4c6dcdc8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_full_comments(subreddit_name, df_sub):\n",
    "    # checkpoint output\n",
    "    print(f'scraping given batch of posts for comment details')\n",
    "    print(f' scrape START time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}', flush=True)\n",
    "    t0 = time.process_time()\n",
    "    \n",
    "    # do the comments detail scrape\n",
    "    df_comm = pd.DataFrame()\n",
    "    for sub in df_sub.index:\n",
    "        sub_id = df_sub.loc[sub]['id']\n",
    "        sub_comms = df_sub.loc[sub]['comments']\n",
    "        keep_commscraping = len(sub_comms)>0\n",
    "        current_start = 1\n",
    "        comms_so_far = 0\n",
    "        while keep_commscraping:\n",
    "            with urllib.request.urlopen(\n",
    "                f'https://api.pushshift.io/reddit/comment/search/'\n",
    "                +f'?limit=1000&sort=asc&link_id={sub_id}&after={current_start}'\n",
    "            ) as url:\n",
    "                data = json.loads(url.read().decode())\n",
    "            if ('data' in data) and len(data['data'])>0:\n",
    "                data = data['data']\n",
    "                df_comm_new = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n",
    "                df_comm = df_comm.append(df_comm_new)\n",
    "                created_utc_now_comm = df_comm_new.tail(1)['created_utc']\n",
    "                current_start = int(created_utc_now_comm + 1)\n",
    "                print(f'new batch comments count: {df_comm_new.shape[0]}, next scanning {current_start}-?', flush=True)\n",
    "                comms_so_far += max(500, df_comm_new.shape[0])\n",
    "                keep_commscraping = (comms_so_far < len(sub_comms))\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                keep_commscraping = False\n",
    "        print(f'finished indexing comments from one submission', flush=True)\n",
    "    print(f'comments count {df_comm.shape[0]}')\n",
    "            \n",
    "    # checkpoint output\n",
    "    print(f'   scrape END time: {time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())}')\n",
    "    print(f'    total DURATION: {time.process_time() - t0}', flush=True)\n",
    "    \n",
    "    # return scraped data\n",
    "    df_comm = df_comm.reset_index(drop=True)\n",
    "    return df_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8dadc-7f2c-4ca1-93b8-4fa1caaa8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comm = scrape_full_comments(subreddit, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850679a-121e-4b29-bc07-4db9ea4523f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b8f87-1fd4-490b-aaab-1da3c260d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2316d15-d4ce-4883-a3e4-d94ae7e0e717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
